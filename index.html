<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8" />
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700;900&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="assets/css/style.css" />

    <link rel="icon" type="images/svg+xml" href="/images/icons/cair.svg">
    <!-- PNG Favicon -->
    <link rel="alternate icon" type="image/png" sizes="32x32" href="/assets/images/icons/favicon-32x32.png">
    <link rel="alternate icon" type="image/png" sizes="16" 16="/assets/images/icons/favicon-16x16.png">

    <!-- ICO Favicon -->

    <meta name="description" content="I'm a second year Computer Science student at Columbia University, and an aspiring AI researcher. " />
    <title> Alper Canberk </title>
</head>

<body>
    <div style="width:1000px;margin: 0px auto;">
        <header id="header" width="400px" style="display:flex;justify-content: space-around;">
            <a href="#profile-intro">Home</a>
            <a href="#updates">Updates</a>
            <a href="#research">Research</a>
            <a href="#projects">Projects</a>
            <a href="#teaching">Teaching</a>

        </header>
        <div id="profile">
            <div id="profile-pic">
                <img src="assets/images/profile-pic-copy.png" />
            </div>
            <div id="profile-intro">
                <div id="profile-name">Alper Canberk</div>
                <div id="profile-email">ac4983 at columbia dot edu</div>
                <p>
                    I'm a rising senior Computer Science student at Columbia University, and an aspiring AI researcher.  </p>

                    <p>I am particularly fascinated by robotics. I not only believe that physical automation can help humanity tackle epic challenges, but I find joy studying the field. </p>

                    <p>Recently, I've been exploring generative video models. I believe that video will be the unifying modality for heterogenous robot data.</p>
                    

                <div>
                    <a href="assets/files/cv.pdf">
                      CV 
                    </a>(outdated) /
                    <a href="https://twitter.com/alpercanbe">
                      Twitter
                    </a> /
                    <a href="https://github.com/alpercanberk">
                        Github
                    </a>
                    </a>
                </div>
            </div>
            <div style="clear: both;"></div>
        </div>
        <div class="section" id="updates">
            <h1>Updates</h1>
            <ul>
                    <li> <b>September 2024</b> Dr. Robot accepted to CoRL 2024!
                    <li> <b>July 2024</b> <a href="https://erasedraw.cs.columbia.edu/">EraseDraw</a> accepted to ECCV 2024 and CVPR AI4CC Workshop!
                    <li> <b>May 2024</b> Back at <a href="https://snap.com/en-US">Snap</a> for a research scientist internship, this time working on large scale generative video models
                        <li> <b>September 2023</b> Joined Columbia CVLab, advised by <a href="https://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a> and mentored by <a href="https://ruoshiliu.github.io/">Ruoshi Liu</a>

                    <li> <b>August 2023</b> Completed my research engineering internship at <a href="https://snap.com/en-US">Snap</a>
                    <li> <b>January 2023</b> <a href="https://clothfunnels.cs.columbia.edu">Cloth Funnels</a>, my first first-author publication, is accepted to ICRA 2023!
                    <li> <b>June 2022</b> Received the Bonomi Scholarship to continue research over the summer.
                    <li> <b>November 2021</b> Joined <a href="https://cair.cs.columbia.edu">Columbia Artificial Intelligence and Robotics Lab</a>! I am fortunate to be mentored by <a href="https://www.cs.columbia.edu/~huy/">Huy Ha</a> and advised by <a href="https://shurans.github.io/">Shuran Song</a>
                    <li> <b>August 2021</b> Started attending Columbia University
                    <li> <b>May 2021</b> <a href="https://arxiv.org/abs/2104.00078">Learning Human Objectives from Sequences of Physical Corrections</a> got published in ICRA 2021.
                    <li> <b>July 2020</b> I spent the summer doing research at <a href='https://iliad.stanford.edu/'>Stanford ILIAD</a> under <a href="https://ai.stanford.edu/~mengxili/">Mengxi Li </a> and <a href="https://dorsa.fyi/">Dorsa Sadigh</a></li>

            </ul>
            <p></p>
            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="research">
            <h1>Research</h1>



            <div>
                <img src="assets/images/drrobot.png" style="height: 12em; width: auto; max-width: 320px; object-fit: contain;" class="research-thumb">
                <a href="" class="research-proj-title">
                   Differentiable Robot Rendering
                </a>
                <p>
                    <b>Alper Canberk*</b>, Ruoshi Liu*, Shuran Song, Carl Vondrick
                    <br> The Conference on Robot Learning (CoRL) 2024 </br>
                    <strong>CoRL 2024 Oral Presentation (6%)</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    <a href="https://drrobot.cs.columbia.edu/">Website</a>


                </p>
            </div>
            <div>
                <img src="assets/images/erasedraw.png" style="height: 12em; " class="research-thumb">
                <a href="https://erasedraw.cs.columbia.edu/" class="research-proj-title">
                    EraseDraw: Learning to Draw Step-by-Step via Erasing
                    Objects from Images
                </a>
                <p>
                    <b>Alper Canberk</b>, Makysym Bondarenko, Ege Ozguroglu, Ruoshi Liu, Carl Vondrick
                    <br> European Conference on Computer Vision 2024 (ECCV), CVPR 2024 AI4CC Workshop </br>
                    <strong>AI4CC Oral Presentation (8%)</strong> &nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    <a href="https://erasedraw.cs.columbia.edu/">Website</a>  &nbsp;&nbsp;&bull;&nbsp;&nbsp; 
                    <a href="https://arxiv.org/pdf/2409.00522v1">ArXiv</a> &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://huggingface.co/alpercanberk/erasedraw">Weights</a> &nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/alpercanberk/erasedraw-data">GitHub</a>
                </p>
            </div>
            <div>
                <a href="https://clothfunnels.cs.columbia.edu/" style="height: 12em;" class="research-thumb">
                    <div class="video-container">
                        <video loop autoplay muted src="assets/images/cf2.mp4" class="thumbnail-video"/>
                    </div>

                </a>
                <a href="https://clothfunnels.cs.columbia.edu/" class="research-proj-title">
                    Cloth Funnels: Canonicalized-Alignment
                    for Multi-Purpose Garment Manipulation
                </a>
                <p>
                    <b>Alper Canberk</b>, Cheng Chi, Huy Ha, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, Shuran Song
                    <br> International Conference on Robotics and Automation (ICRA), May 2023 </br>
                    <a href="https://clothfunnels.cs.columbia.edu/">Website</a> &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://arxiv.org/abs/2210.09347">ArXiv</a> &nbsp;&nbsp;&bull;&nbsp;&nbsp; <a href="https://github.com/columbia-ai-robotics/cloth-funnels">GitHub</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <!-- <a href="https://arxiv.org/abs/2104.05177">ArXiv</a> &nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                    <a href="https://www.youtube.com/watch?v=TkUn0b7mbj0">Video</a>
                </p>
            </div>
            <div>
                <a href="" style="height: 12em;" class="research-thumb">
                    <!-- <img width="320" height="auto" src="https://iliad.stanford.edu/images/publications/li2021learning.png" /> -->
                    <div class="video-container">
                        <video loop autoplay muted src="assets/images/iliad_hri_corrections.mp4" class="thumbnail-video"/>
                    </div>

                    <a href="https://arxiv.org/abs/2104.00078" class="research-proj-title">
                    Learning Human Objectives from Sequences of Physical Corrections
                </a>
                    <p>
                        Mengxi Li, <b>Alper Canberk</b>, Dylan P. Losey, Dorsa Sadigh
                        <br>International Conference on Robotics and Automation (ICRA), May 2021 </br>
                    <a href="https://arxiv.org/abs/2104.00078">ArXiv</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/Stanford-ILIAD/multiagent_correction">GitHub</a>

                    </p>
            </div>

            <!-- <div>
                <p>* indicates equal contribution</p>
            </div> -->

            <div style="clear: both;"></div>
        </div>
        <div class="divider"></div>
        <div style="display:flex;flex-direction: column;" id="projects">
            <h1>Projects</h1>

            <!-- <div id="projects"> -->
                <!-- <img src="assets/images/review.jpeg" alt=""> -->
                <div>
                    <a href="" style="height: 12em;" class="research-thumb">
                        <img width="320" height="auto" src="assets/images/videounderstanding.png" />
                        <a href="" class="research-proj-title">
                        State of Video Understanding Models as Robot Task Verifiers – Topics in Robot Learning Final Project 
                    </a>
                        <p>
                            In this report, we examine the effects of camera angle, video input segment, the object of interest, and background on the ability of video understanding models to accurately classify robot actions. Additionally, we explore how to best utilize the action classifier's output in robotic manipulation. 
                            
                            <br><b>December 2022</b></br>
                            <a href="assets/files/video_understanding_models.pdf">PDF</a>
    
                        <!-- <a href="https://alpercanberk.medium.com/how-to-make-a-unity-app-for-oculus-quest-2-part-2-hand-tracking-whiteboard-e3e97887d3c8">Medium</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <!-- <a href="https://github.com/alpercanberk/Handtracking-Whiteboard-Oculus">GitHub</a> -->
    
                        </p>
                </div>
                <div>
                    <a href="" style="height: 12em;" class="research-thumb">
                        <img width="320" height="auto" src="assets/images/whiteboard.gif" />
                        <a href="" class="research-proj-title">
                        Oculus VR Handtracking Whiteboard
                    </a>
                        <p>
                            A Unity VR app where the user can dynamically create, move, and resize whiteboards (on a real wall if they prefer), and can write on it using their real fingers.
                            <br><b>June 2021</b></br>
                            <a href="https://alpercanberk.medium.com/how-to-make-a-unity-app-for-oculus-quest-2-part-2-hand-tracking-whiteboard-e3e97887d3c8">Medium</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                            <a href="https://github.com/alpercanberk/Handtracking-Whiteboard-Oculus">GitHub</a>
                        </p>
                </div>

                <div>
                    <a href="" style="height: 12em;" class="research-thumb">
                        <img width="320" height="auto" src="assets/images/nexus-img.png" />
                        <a href="" class="research-proj-title">
                        Nexus Virtual Graduation
                    </a>
                        <p>
                            A real time multiplayer platform for my high school’s virtual graduation experience on Unity. Features include: Google Maps 3D location scraping, student avatar creation, free movement around 3d model of the real campus, P2P Real-time spacial voice and
                            video chat, in-game YouTube with adjustable screen, and inter-character interactions such as diploma giving, online event sign-ups.
                            <br><b>May 2020</b></br>
                        <!-- <a href="https://alpercanberk.medium.com/how-to-make-a-unity-app-for-oculus-quest-2-part-2-hand-tracking-whiteboard-e3e97887d3c8">Medium</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp; -->
                        <a href="http://www.thenexus.cloud/">Website</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://www.youtube.com/watch?v=0xlcuqARsis">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                        <a href="https://github.com/Nexus-Virtual-Events">GitHub</a>
    
    
    
                        </p>
                </div>
               
                
                
            </div>
            <div class="divider"></div>
            <div style="display:flex;flex-direction: column;" id="research">
                <div id="teaching">
                    <h1>Teaching & Service</h1>
                    <!-- <img src="assets/images/review.jpeg" alt=""> -->
                    <div>
                        <ul style="list-style-type: circle;">
                            <li>
                                <b>Teaching Assistant</b> for CS 4771 Machine Learning with Nakul Verma - Spring 2024
                            </li>
                            <li>
                                <b>Teaching Assistant</b> for CS 4231 Analysis of Algorithms I with Christos Papadimitriou - Spring 2024
                            </li>
                            <li>
                                <b>Teaching Assistant</b> for CS 4231 Analysis of Algorithms I with Tim Roughgarden – Fall 2023
                            </li>
                            <li>
                                <b>Conference Reviewer</b> for IEEE International Conference on Robotics and Automation (ICRA) – Spring 2023
                            </li>
                            <li>
                            <b>Student Volunteer </b>for Robotics: Science and Systems Conference (RSS) – Summer 2022
                            </li>
                           
                        </ul>
                    </div>
                </div>
    
            </div>
            <div style="height: 100px"> &nbsp;</div>
        </div>               


        <div style="text-align: center; margin-top: 2em; margin-bottom: 2em;"> This website is largely borrowed from <a href="https://www.cs.columbia.edu/~huy/">Huy Ha</a>'s website (don't worry, I will give it back soon) </div>

   

        <!-- <div style="clear: both; height: 100px"> &nbsp;</div> -->

        <!-- <div class="divider"></div>
        <div style="display:flex;flex-direction: column;">
            <h1>Projects</h1>
            <div>
                <a href="soft-creatures/index.html" class="research-thumb"><img
                        src="soft-creatures/gifs/elizabeth-hickey-small.gif" alt=""></a>
                <a href="soft-creatures/index.html" class="research-proj-title">Coevolution of Morphology and Policy
                    Implicit Neural Functions</a>
                <p>
                    <b>Huy Ha</b><br>
                    <a href="https://github.com/huy-ha/coevolving-neural-networks">Github</a>,
                    <a href="soft-creatures/index.html">Website</a>
                </p>
            </div>
            <div style="clear: both;"></div>
        </div> -->
    </div>
    <!-- <div class="divider"></div> -->

    <!-- <script src="js/pixi.min.js"></script> -->
    <!-- <script src="js/profile-pic.js"></script> -->
</body>


</html>